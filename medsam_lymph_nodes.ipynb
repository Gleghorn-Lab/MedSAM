{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 0:\n",
    "!pip install -r requirements.txt -U\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e6caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Authentication\n",
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import huggingface_hub\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "from utils.demo import BboxPromptDemo\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "\n",
    "# Login to Hugging Face\n",
    "hf_token = input(\"Enter your Hugging Face token: \")\n",
    "huggingface_hub.login(token=hf_token)\n",
    "\n",
    "print(\"Authentication successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823cfe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Dataset and Model (From Hugging Face)\n",
    "\n",
    "print(\"Loading lymph node dataset...\")\n",
    "dataset_input = input(\"Please enter dataset path: \")\n",
    "\n",
    "try:\n",
    "    # First, analyze the dataset structure\n",
    "    temp_dataset = load_dataset(dataset_input, token=True)\n",
    "    \n",
    "    print(\"ðŸ” Dataset Analysis:\")\n",
    "    print(f\"Available splits: {list(temp_dataset.keys())}\")\n",
    "    for split_name in temp_dataset.keys():\n",
    "        print(f\"  - {split_name}: {len(temp_dataset[split_name])} items\")\n",
    "        if len(temp_dataset[split_name]) > 0:\n",
    "            print(f\"    Features: {temp_dataset[split_name].features}\")\n",
    "    \n",
    "    # Choose which split to use as 'train'\n",
    "    chosen_split = input(f\"Which split to use as 'train'? {list(temp_dataset.keys())}: \")\n",
    "    \n",
    "    if chosen_split in temp_dataset:\n",
    "        print(f\"Will use '{chosen_split}' split\")\n",
    "        \n",
    "        # Load the chosen split and create 'train' alias\n",
    "        image_dataset = temp_dataset\n",
    "        image_dataset['train'] = image_dataset[chosen_split]\n",
    "        \n",
    "        print(\"Dataset info:\")\n",
    "        print(f\"Available splits: {list(image_dataset.keys())}\")\n",
    "        print(f\"Number of train images: {len(image_dataset['train'])}\")\n",
    "        print(f\"Features: {image_dataset['train'].features}\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\" Split '{chosen_split}' not found in dataset\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"\\n Troubleshooting:\")\n",
    "    print(\"1. Check if the dataset repository exists and is accessible\")\n",
    "    print(\"2. Verify your HuggingFace token has the correct permissions\")\n",
    "    print(\"3. Make sure the dataset path is correct\")\n",
    "    print(\"4. Try a different split name if available\")\n",
    "    print(\"\\n Cannot continue without valid dataset. Please fix the issue and try again.\")\n",
    "    raise  # Re-raise the error to stop execution\n",
    "\n",
    "# Load MedSAM model from Hugging Face\n",
    "print(\"\\nLoading MedSAM model from Hugging Face...\")\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "try:\n",
    "    # Use your manually downloaded checkpoint if already downloaded\n",
    "    MedSAM_CKPT_PATH = \"/home/medsam-vit-b/medsam_vit_b.pth\"\n",
    "    \n",
    "    # #OR USE THIS section to download the model directly from Hugging Face into cache\n",
    "    # MedSAM_CKPT_PATH = hf_hub_download(\n",
    "    #     repo_id=\"GleghornLab/medsam-vit-b\",\n",
    "    #     filename=\"medsam_vit_b.pth\",\n",
    "    #     token=True\n",
    "    # )\n",
    "    \n",
    "    print(f\"Model downloaded to: {MedSAM_CKPT_PATH}\")\n",
    "    \n",
    "    # Load the model\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "    medsam_model = medsam_model.to(device)\n",
    "    medsam_model.eval()\n",
    "    \n",
    "    print(f\"MedSAM model loaded successfully on {device}\")\n",
    "    \n",
    "    # Show device info\n",
    "    print(f\"ðŸ–¥ï¸ Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"\\nModel troubleshooting:\")\n",
    "    print(\"1. Check if the model repository 'GleghornLab/medsam-vit-b' exists\")\n",
    "    print(\"2. Verify you have access permissions to this repository\")\n",
    "    print(\"3. Ensure your HuggingFace token has the correct permissions\")\n",
    "    print(\"\\n Cannot continue without valid model. Please fix the issue and try again.\")\n",
    "    raise  # Re-raise the error to stop execution\n",
    "\n",
    "print(\"\\n Setup complete! Dataset and model loaded successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "# image_dataset = load_dataset(dataset_input, token=True)\n",
    "\n",
    "# print(\"Dataset info:\")\n",
    "# print(f\"Available splits: {list(image_dataset.keys())}\")\n",
    "# print(f\"Number of train images: {len(image_dataset['train'])}\")\n",
    "# print(f\"Features: {image_dataset['train'].features}\")\n",
    "\n",
    "# # Load MedSAM model from Hugging Face\n",
    "# print(\"\\nLoading MedSAM model from Hugging Face...\")\n",
    "# from huggingface_hub import hf_hub_download\n",
    "\n",
    "# # Download model from Hugging Face repository\n",
    "# MedSAM_CKPT_PATH = hf_hub_download(\n",
    "#     repo_id=\"GleghornLab/medsam-vit-b\",\n",
    "#     filename=\"medsam_vit_b.pth\",\n",
    "#     token=True\n",
    "# )\n",
    "\n",
    "# print(f\"Model downloaded to: {MedSAM_CKPT_PATH}\")\n",
    "\n",
    "# # Load the model\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "# medsam_model = medsam_model.to(device)\n",
    "# medsam_model.eval()\n",
    "# print(f\"MedSAM model loaded successfully on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5150d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Optimized MedSAM Interface - Starts Small for Better Performance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import LassoSelector, Button, Slider\n",
    "from matplotlib.path import Path\n",
    "import torch\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "class AdjustableViewLassoMedSAMInterface:\n",
    "    def __init__(self, model, dataset):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.original_image = None  # Original for MedSAM processing\n",
    "        self.display_image = None   # Adjusted for viewing only\n",
    "        self.lasso_points = []\n",
    "        self.current_mask = None\n",
    "        self.fig = None\n",
    "        self.ax_image = None\n",
    "        \n",
    "        # Display adjustment parameters (viewing only)\n",
    "        self.brightness = 1.0\n",
    "        self.contrast = 1.0\n",
    "        self.size_factor = 0.3  # Start much smaller for performance\n",
    "        \n",
    "    def load_image(self, image_index):\n",
    "        \"\"\"Load an image with viewing adjustments and lasso selection\"\"\"\n",
    "        if image_index >= len(self.dataset['train']):\n",
    "            print(f\"Index {image_index} out of range. Max: {len(self.dataset['train'])-1}\")\n",
    "            return\n",
    "            \n",
    "        sample = self.dataset['train'][image_index]\n",
    "        pil_image = sample['image']\n",
    "        \n",
    "        print(f\"Loading Image #{image_index + 1}\")\n",
    "        print(f\"Original size: {pil_image.size}\")\n",
    "        \n",
    "        # Store original image for MedSAM processing (never modified)\n",
    "        self.original_image = np.array(pil_image)\n",
    "        self.original_pil_image = pil_image\n",
    "        \n",
    "        # Start with small size for better performance\n",
    "        self.brightness = 1.0\n",
    "        self.contrast = 1.0\n",
    "        self.size_factor = 0.3  # Start at 30% for 5632x5632 images\n",
    "        \n",
    "        print(f\"Display will start at {self.size_factor:.1%} size for smooth interaction\")\n",
    "        \n",
    "        # Initialize display image\n",
    "        self.update_display_image()\n",
    "        \n",
    "        # Create the interactive interface\n",
    "        self.setup_adjustable_interface()\n",
    "        \n",
    "    def update_display_image(self):\n",
    "        \"\"\"Update display image with viewing adjustments (original unchanged)\"\"\"\n",
    "        adjusted_image = self.original_pil_image.copy()\n",
    "        \n",
    "        # Apply size first (most important for performance)\n",
    "        new_size = tuple(int(dim * self.size_factor) for dim in adjusted_image.size)\n",
    "        adjusted_image = adjusted_image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Apply brightness/contrast to smaller image\n",
    "        if self.brightness != 1.0:\n",
    "            enhancer = ImageEnhance.Brightness(adjusted_image)\n",
    "            adjusted_image = enhancer.enhance(self.brightness)\n",
    "        \n",
    "        if self.contrast != 1.0:\n",
    "            enhancer = ImageEnhance.Contrast(adjusted_image)\n",
    "            adjusted_image = enhancer.enhance(self.contrast)\n",
    "        \n",
    "        self.display_image = np.array(adjusted_image)\n",
    "        \n",
    "    def setup_adjustable_interface(self):\n",
    "        \"\"\"Setup the interface with viewing adjustments and lasso selection\"\"\"\n",
    "        if self.fig is not None:\n",
    "            plt.close(self.fig)\n",
    "        \n",
    "        self.fig = plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Main image axes\n",
    "        self.ax_image = plt.axes([0.1, 0.3, 0.6, 0.6])\n",
    "        self.ax_image.imshow(self.display_image)\n",
    "        self.ax_image.set_title(f\"Lasso Tool (Display: {self.display_image.shape[:2]}, Original: {self.original_image.shape[:2]})\")\n",
    "        \n",
    "        # Simple lasso selector\n",
    "        self.lasso = LassoSelector(self.ax_image, self.on_lasso_select)\n",
    "        \n",
    "        self.add_viewing_sliders()\n",
    "        self.add_control_buttons()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ðŸš€ OPTIMIZED Interface:\")\n",
    "        print(f\"ðŸ“± Display: {self.display_image.shape[:2]} (for smooth interaction)\")\n",
    "        print(f\"ðŸ”¬ Processing: {self.original_image.shape[:2]} (for accuracy)\")\n",
    "        print(\"ðŸ’¡ Use View Size slider to zoom in when needed\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    def add_viewing_sliders(self):\n",
    "        \"\"\"Add sliders for viewing adjustments only\"\"\"\n",
    "        # Size slider - starts at current small size\n",
    "        ax_size = plt.axes([0.75, 0.8, 0.15, 0.03])\n",
    "        self.slider_size = Slider(ax_size, 'View Size', 0.1, 1.0, valinit=self.size_factor, valfmt='%.1f')\n",
    "        self.slider_size.on_changed(self.update_view_size)\n",
    "        \n",
    "        # Brightness slider\n",
    "        ax_brightness = plt.axes([0.75, 0.75, 0.15, 0.03])\n",
    "        self.slider_brightness = Slider(ax_brightness, 'Brightness', 0.5, 2.0, valinit=1.0, valfmt='%.1f')\n",
    "        self.slider_brightness.on_changed(self.update_view_brightness)\n",
    "        \n",
    "        # Contrast slider\n",
    "        ax_contrast = plt.axes([0.75, 0.7, 0.15, 0.03])\n",
    "        self.slider_contrast = Slider(ax_contrast, 'Contrast', 0.5, 2.0, valinit=1.0, valfmt='%.1f')\n",
    "        self.slider_contrast.on_changed(self.update_view_contrast)\n",
    "        \n",
    "        # Performance info\n",
    "        ax_info = plt.axes([0.75, 0.65, 0.15, 0.03])\n",
    "        ax_info.text(0.1, 0.5, f\"Current: {self.display_image.shape[1]}x{self.display_image.shape[0]}\", \n",
    "                    transform=ax_info.transAxes, fontsize=9)\n",
    "        ax_info.set_xticks([])\n",
    "        ax_info.set_yticks([])\n",
    "        \n",
    "    def add_control_buttons(self):\n",
    "        \"\"\"Add control buttons\"\"\"\n",
    "        ax_test = plt.axes([0.1, 0.15, 0.12, 0.04])\n",
    "        ax_accept = plt.axes([0.23, 0.15, 0.12, 0.04])\n",
    "        ax_clear = plt.axes([0.36, 0.15, 0.12, 0.04])\n",
    "        ax_new_region = plt.axes([0.49, 0.15, 0.12, 0.04])\n",
    "        \n",
    "        self.btn_test = Button(ax_test, 'Test Segmentation')\n",
    "        self.btn_accept = Button(ax_accept, 'Accept')\n",
    "        self.btn_clear = Button(ax_clear, 'Clear')\n",
    "        self.btn_new = Button(ax_new_region, 'New Region')\n",
    "        \n",
    "        self.btn_test.on_clicked(self.test_segmentation)\n",
    "        self.btn_accept.on_clicked(self.accept_segmentation)\n",
    "        self.btn_clear.on_clicked(self.clear_selection)\n",
    "        self.btn_new.on_clicked(self.new_region)\n",
    "        \n",
    "    def update_view_size(self, val):\n",
    "        \"\"\"Update viewing size only\"\"\"\n",
    "        self.size_factor = val\n",
    "        self.refresh_display()\n",
    "        \n",
    "    def update_view_brightness(self, val):\n",
    "        self.brightness = val\n",
    "        self.refresh_display()\n",
    "        \n",
    "    def update_view_contrast(self, val):\n",
    "        self.contrast = val\n",
    "        self.refresh_display()\n",
    "        \n",
    "    def refresh_display(self):\n",
    "        \"\"\"Refresh the display efficiently\"\"\"\n",
    "        self.update_display_image()\n",
    "        self.ax_image.clear()\n",
    "        self.ax_image.imshow(self.display_image)\n",
    "        self.ax_image.set_title(f\"Lasso Tool (Display: {self.display_image.shape[:2]}, Original: {self.original_image.shape[:2]})\")\n",
    "        \n",
    "        # Recreate lasso selector\n",
    "        self.lasso = LassoSelector(self.ax_image, self.on_lasso_select)\n",
    "        \n",
    "        # Clear selection when view changes\n",
    "        self.lasso_points = []\n",
    "        self.current_mask = None\n",
    "        \n",
    "        self.fig.canvas.draw_idle()\n",
    "        \n",
    "    def on_lasso_select(self, verts):\n",
    "        \"\"\"Handle lasso selection - scale coordinates to original image\"\"\"\n",
    "        if len(verts) > 2:\n",
    "            # Scale display coordinates to original image coordinates\n",
    "            scale_factor = 1.0 / self.size_factor\n",
    "            self.lasso_points = [(x * scale_factor, y * scale_factor) for x, y in verts]\n",
    "            print(f\"âœ“ Lasso drawn with {len(verts)} points (scaled to original coordinates)\")\n",
    "            \n",
    "    def test_segmentation(self, event):\n",
    "        \"\"\"Test segmentation using ORIGINAL full-resolution image\"\"\"\n",
    "        if not self.lasso_points:\n",
    "            print(\"Please draw a lasso first!\")\n",
    "            return\n",
    "            \n",
    "        print(\"ðŸ”¬ Running MedSAM on ORIGINAL full-resolution image...\")\n",
    "        \n",
    "        # Convert lasso to bounding box on original coordinates\n",
    "        points = np.array(self.lasso_points)\n",
    "        x_min, y_min = points.min(axis=0).astype(int)\n",
    "        x_max, y_max = points.max(axis=0).astype(int)\n",
    "        \n",
    "        # Ensure bbox is within bounds\n",
    "        x_min = max(0, x_min)\n",
    "        y_min = max(0, y_min)\n",
    "        x_max = min(self.original_image.shape[1], x_max)\n",
    "        y_max = min(self.original_image.shape[0], y_max)\n",
    "        \n",
    "        bbox = np.array([x_min, y_min, x_max, y_max])\n",
    "        print(f\"Bounding box on original image: {bbox}\")\n",
    "        \n",
    "        # Get segmentation from MedSAM\n",
    "        self.current_mask = self.get_medsam_segmentation(bbox)\n",
    "        self.current_mask = self.refine_with_lasso(self.current_mask)\n",
    "        \n",
    "        self.show_segmentation_result()\n",
    "        \n",
    "    def get_medsam_segmentation(self, bbox):\n",
    "        \"\"\"Get segmentation from MedSAM using ORIGINAL image\"\"\"\n",
    "        img_tensor = torch.tensor(self.original_image).float()\n",
    "        if len(img_tensor.shape) == 3:\n",
    "            img_tensor = img_tensor.permute(2, 0, 1)\n",
    "        \n",
    "        img_tensor = img_tensor.unsqueeze(0).to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_embeddings = self.model.image_encoder(img_tensor)\n",
    "            bbox_tensor = torch.tensor(bbox).float().unsqueeze(0).to(self.model.device)\n",
    "            \n",
    "            sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "                points=None, boxes=bbox_tensor, masks=None,\n",
    "            )\n",
    "            \n",
    "            masks, iou_predictions = self.model.mask_decoder(\n",
    "                image_embeddings=image_embeddings,\n",
    "                image_pe=self.model.prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "            \n",
    "            mask = masks[0, 0].cpu().numpy()\n",
    "            \n",
    "        print(\" MedSAM segmentation complete!\")\n",
    "        return mask > 0.5\n",
    "        \n",
    "    def refine_with_lasso(self, mask):\n",
    "        \"\"\"Refine mask using lasso path on original coordinates\"\"\"\n",
    "        if not self.lasso_points:\n",
    "            return mask\n",
    "            \n",
    "        path = Path(self.lasso_points)\n",
    "        h, w = mask.shape\n",
    "        y, x = np.mgrid[:h, :w]\n",
    "        points = np.column_stack((x.ravel(), y.ravel()))\n",
    "        inside_lasso = path.contains_points(points).reshape(h, w)\n",
    "        return mask & inside_lasso\n",
    "        \n",
    "    def show_segmentation_result(self):\n",
    "        \"\"\"Display segmentation result scaled to display\"\"\"\n",
    "        if self.current_mask is None:\n",
    "            return\n",
    "            \n",
    "        # Scale mask to display size\n",
    "        mask_pil = Image.fromarray((self.current_mask * 255).astype(np.uint8))\n",
    "        display_size = self.display_image.shape[:2][::-1]\n",
    "        scaled_mask_pil = mask_pil.resize(display_size, Image.Resampling.NEAREST)\n",
    "        display_mask = np.array(scaled_mask_pil) > 127\n",
    "            \n",
    "        # Show result\n",
    "        self.ax_image.clear()\n",
    "        self.ax_image.imshow(self.display_image)\n",
    "        \n",
    "        # Overlay mask\n",
    "        masked = np.ma.masked_where(~display_mask, display_mask)\n",
    "        self.ax_image.imshow(masked, alpha=0.6, cmap='Reds')\n",
    "        \n",
    "        # Show lasso outline\n",
    "        if self.lasso_points:\n",
    "            display_lasso = [(x * self.size_factor, y * self.size_factor) for x, y in self.lasso_points]\n",
    "            lasso_array = np.array(display_lasso)\n",
    "            self.ax_image.plot(lasso_array[:, 0], lasso_array[:, 1], 'r-', linewidth=2)\n",
    "            \n",
    "        self.ax_image.set_title(\"Segmentation Result (processed on full-resolution)\")\n",
    "        self.fig.canvas.draw_idle()\n",
    "        \n",
    "    def accept_segmentation(self, event):\n",
    "        \"\"\"Accept the current segmentation\"\"\"\n",
    "        if self.current_mask is None:\n",
    "            print(\" No segmentation to accept!\")\n",
    "            return\n",
    "            \n",
    "        print(\"Segmentation accepted!\")\n",
    "        print(f\"Mask: {self.current_mask.shape}, Pixels: {np.sum(self.current_mask)}\")\n",
    "        self.new_region(event)\n",
    "        \n",
    "    def clear_selection(self, event):\n",
    "        \"\"\"Clear the current selection\"\"\"\n",
    "        self.lasso_points = []\n",
    "        self.current_mask = None\n",
    "        \n",
    "        self.ax_image.clear()\n",
    "        self.ax_image.imshow(self.display_image)\n",
    "        self.ax_image.set_title(f\"Lasso Tool (Display: {self.display_image.shape[:2]}, Original: {self.original_image.shape[:2]})\")\n",
    "        \n",
    "        self.lasso = LassoSelector(self.ax_image, self.on_lasso_select)\n",
    "        self.fig.canvas.draw_idle()\n",
    "        print(\"ðŸ§¹ Selection cleared\")\n",
    "        \n",
    "    def new_region(self, event):\n",
    "        \"\"\"Start selecting a new region\"\"\"\n",
    "        self.lasso_points = []\n",
    "        self.lasso = LassoSelector(self.ax_image, self.on_lasso_select)\n",
    "        self.ax_image.set_title(\" Draw another region\")\n",
    "        print(\"Ready for new region selection\")\n",
    "        \n",
    "    def preview_images(self, start_idx=0, num_images=9):\n",
    "        \"\"\"Preview images at reduced resolution\"\"\"\n",
    "        print(f\"Preview: Images {start_idx} to {start_idx + num_images - 1}\")\n",
    "        \n",
    "        fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "        axs = axs.flatten()\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            idx = start_idx + i\n",
    "            if idx < len(self.dataset['train']):\n",
    "                image = self.dataset['train'][idx]['image']\n",
    "                # Show at reduced size for preview\n",
    "                if max(image.size) > 400:\n",
    "                    preview_size = tuple(int(dim * 400 / max(image.size)) for dim in image.size)\n",
    "                    image = image.resize(preview_size, Image.Resampling.LANCZOS)\n",
    "                \n",
    "                axs[i].imshow(image)\n",
    "                axs[i].set_title(f\"Image #{idx + 1}\")\n",
    "                axs[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3766a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Initialize Adjustable View Interface\n",
    "interface = AdjustableViewLassoMedSAMInterface(medsam_model, image_dataset)\n",
    "\n",
    "# Preview images\n",
    "interface.preview_images(0, 9)\n",
    "\n",
    "print(\"\\nAdjustable View Lasso MedSAM Commands:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"â€¢ interface.load_image(n) - Load image with viewing adjustments\")\n",
    "print(\"â€¢ interface.preview_images(start, count) - Preview images\")\n",
    "\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"âœ“ Viewing adjustments: size, brightness, contrast (temporary)\")\n",
    "print(\"âœ“ MedSAM processing: always uses original unmodified image\")\n",
    "print(\"âœ“ Coordinate mapping: lasso coordinates mapped to original image\")\n",
    "print(\"âœ“ Visual feedback: see adjustments while maintaining data integrity\")\n",
    "\n",
    "print(\"\\nHow to Use:\")\n",
    "print(\"1. interface.load_image(5)\")\n",
    "print(\"2. Adjust viewing parameters as needed\")\n",
    "print(\"3. Draw a lasso around your region\")\n",
    "print(\"4. Click 'Test Segmentation' (processes original image)\")\n",
    "print(\"5. Click 'Accept' or 'Clear' to continue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce65bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Quick Functions\n",
    "def load_image(n):\n",
    "    return interface.load_image(n)\n",
    "\n",
    "def preview(start=0):\n",
    "    interface.preview_images(start, 9)\n",
    "\n",
    "print(\"Quick Commands:\")\n",
    "print(\"â€¢ load_image(n)\")\n",
    "print(\"â€¢ preview()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36bd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image(0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
