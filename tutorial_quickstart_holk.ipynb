{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment Anything in Medical Images ([colab](https://colab.research.google.com/drive/1N4wv9jljtEZ_w-f92iOLXCdkD-KJlsJH?usp=sharing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -U\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the local device:\n",
    "- Create a fresh environment `conda create -n medsam python=3.10 -y` and activate it `conda activate medsam`\n",
    "- Install \n",
    "pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126\n",
    "- Continue to next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-trained model\n",
    "\n",
    "Please download the checkpoint [here](https://drive.google.com/drive/folders/1ETWmi4AiniJeWOt6HAsYgTjYv_fkgzoN?usp=drive_link). This pre-trained model can be directed loaded with SAM's checkpoint loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "from segment_anything import sam_model_registry\n",
    "from utils.demo import BboxPromptDemo\n",
    "MedSAM_CKPT_PATH = \"/home/medsam-vit-b/medsam_vit_b.pth\"\n",
    "device = \"cuda:0\"\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "medsam_model = medsam_model.to(device)\n",
    "medsam_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in tutorial_quickstart_holk.ipynb, new cell\n",
    "%matplotlib widget\n",
    "import os, numpy as np, torch\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import RectangleSelector\n",
    "from medsam_inference import medsam_inference\n",
    "\n",
    "# CONFIGURE paths & HF repo\n",
    "DATASET_NAME = \"GleghornLab/full_LN_6-1\"\n",
    "SPLIT        = \"train\"\n",
    "ROOT         = \"/home/MedSAM/data/follicle/train\"\n",
    "IMG_DIR      = os.path.join(ROOT, \"images\"); os.makedirs(IMG_DIR, exist_ok=True)\n",
    "MASK_DIR     = os.path.join(ROOT, \"masks\");  os.makedirs(MASK_DIR, exist_ok=True)\n",
    "\n",
    "ds = load_dataset(DATASET_NAME, token=True)[SPLIT]\n",
    "\n",
    "# build per‐section index map\n",
    "section_map = {}\n",
    "for i, ex in enumerate(ds):\n",
    "    sec = ex[\"section\"]  # or whatever your field is\n",
    "    section_map.setdefault(sec, []).append(i)\n",
    "\n",
    "# now pick how many per section go to train/val/test:\n",
    "train_idx = []\n",
    "val_idx   = []\n",
    "test_idx  = []\n",
    "for sec, inds in section_map.items():\n",
    "    # e.g. first 6 → train, next 1 → val, next 1 → test\n",
    "    train_idx += inds[:6]\n",
    "    val_idx   += inds[6:7]\n",
    "    test_idx  += inds[7:8]\n",
    "\n",
    "print(\"Train:\", len(train_idx), \"Val:\", len(val_idx), \"Test:\", len(test_idx))\n",
    "\n",
    "\n",
    "def annotate_and_save_multi(idx):\n",
    "    pil_img = ds[idx][\"image\"]\n",
    "    img = np.array(pil_img); H, W = img.shape[:2]\n",
    "    bboxes = []\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.imshow(img); ax.set_title(f\"Image #{idx}: draw boxes, ENTER when done\")\n",
    "\n",
    "    def onselect(e0,e1):\n",
    "        x1,y1 = int(e0.xdata), int(e0.ydata)\n",
    "        x2,y2 = int(e1.xdata), int(e1.ydata)\n",
    "        bboxes.append([x1,y1,x2,y2])\n",
    "        ax.add_patch(plt.Rectangle((x1,y1),x2-x1,y2-y1,\n",
    "                                   edgecolor=\"yellow\",fill=False,lw=2))\n",
    "        fig.canvas.draw()\n",
    "\n",
    "    sel = RectangleSelector(ax, onselect, drawtype=\"box\",\n",
    "                            useblit=True, button=[1],\n",
    "                            minspanx=5, minspany=5)\n",
    "    fig.canvas.mpl_connect(\"key_press_event\",\n",
    "                           lambda evt: plt.close(fig) if evt.key==\"enter\" else None)\n",
    "    plt.show()\n",
    "\n",
    "    if not bboxes:\n",
    "        print(\"no boxes → skipped\"); return\n",
    "\n",
    "    # preprocess & encode once\n",
    "    img1024 = transform.resize(img, (1024,1024),\n",
    "                               order=3, preserve_range=True,\n",
    "                               anti_aliasing=True).astype(np.uint8)\n",
    "    norm = (img1024 - img1024.min())/np.clip(img1024.max()-img1024.min(),1e-8,None)\n",
    "    tensor = (torch.tensor(norm).float()\n",
    "                        .permute(2,0,1)\n",
    "                        .unsqueeze(0)\n",
    "                        .to(device))\n",
    "    with torch.no_grad():\n",
    "        embedding = medsam_model.image_encoder(tensor)\n",
    "\n",
    "    # run and save one mask per box\n",
    "    for i, box in enumerate(bboxes):\n",
    "        box1024 = np.array([box]) / np.array([W,H,W,H]) * 1024\n",
    "        mask = medsam_inference(medsam_model, embedding, box1024, H, W)\n",
    "        name = f\"{idx:04d}_{i:02d}.png\"\n",
    "        pil_img.save(os.path.join(IMG_DIR,  name))\n",
    "        Image.fromarray((mask*255).astype(\"uint8\"))\\\n",
    "             .save(os.path.join(MASK_DIR, name))\n",
    "        print(\"✓\", name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the next cell, annotate multiple objects on image 0\n",
    "annotate_and_save_multi(0)# draw multiple boxes, ENTER to finish\n",
    "\n",
    "# then when that finishes, call for image 1, 2, …\n",
    "#annotate_and_save_multi(1)# then move on to image #1, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
