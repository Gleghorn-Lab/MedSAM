{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment Anything in Medical Images ([colab](https://colab.research.google.com/drive/1N4wv9jljtEZ_w-f92iOLXCdkD-KJlsJH?usp=sharing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -U\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the local device:\n",
    "- Create a fresh environment `conda create -n medsam python=3.10 -y` and activate it `conda activate medsam`\n",
    "- Install \n",
    "pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126\n",
    "- Continue to next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-trained model\n",
    "\n",
    "Please download the checkpoint [here](https://drive.google.com/drive/folders/1ETWmi4AiniJeWOt6HAsYgTjYv_fkgzoN?usp=drive_link). This pre-trained model can be directed loaded with SAM's checkpoint loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "from segment_anything import sam_model_registry\n",
    "from utils.demo import BboxPromptDemo\n",
    "MedSAM_CKPT_PATH = \"/home/medsam-vit-b/medsam_vit_b.pth\"\n",
    "device = \"cuda:0\"\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "medsam_model = medsam_model.to(device)\n",
    "medsam_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from skimage import transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import RectangleSelector\n",
    "\n",
    "# pure‐PyTorch inference fn (no Qt)\n",
    "from medsam_inference import medsam_inference  \n",
    "\n",
    "# ─── USER CONFIG ───────────────────────────────────────────────────────────────\n",
    "DATASET_NAME = \"GleghornLab/full_LN_6-1\"\n",
    "SPLIT        = \"train\"    # will fall back to first split if ‘train’ not present\n",
    "\n",
    "# where to save your TRAIN masks/images\n",
    "ROOT        = \"/home/MedSAM/data/follicle/train\"\n",
    "IMG_DIR     = os.path.join(ROOT, \"images\"); os.makedirs(IMG_DIR, exist_ok=True)\n",
    "MASK_DIR    = os.path.join(ROOT, \"masks\");  os.makedirs(MASK_DIR, exist_ok=True)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# load HF dataset with fallback if no ‘train’ split\n",
    "raw = load_dataset(DATASET_NAME, token=True)\n",
    "if isinstance(raw, DatasetDict):\n",
    "    if SPLIT in raw:\n",
    "        ds = raw[SPLIT]\n",
    "    else:\n",
    "        first = list(raw.keys())[0]\n",
    "        print(f\"split '{SPLIT}' not found, using '{first}' instead\")\n",
    "        ds = raw[first]\n",
    "else:\n",
    "    ds = raw\n",
    "print(f\"Loaded {len(ds)} examples from dataset.\")\n",
    "\n",
    "# build per‐section index map\n",
    "section_map = {}\n",
    "for i, ex in enumerate(ds):\n",
    "    sec = ex.get(\"section\", \"unknown\")\n",
    "    section_map.setdefault(sec, []).append(i)\n",
    "print(\"Available sections:\", list(section_map.keys()))\n",
    "\n",
    "# decide your per‐section split (here 75% train, 12.5%  val, 12.5% test)\n",
    "train_idx = []\n",
    "val_idx   = []\n",
    "test_idx  = []\n",
    "\n",
    "for sec, inds in section_map.items():\n",
    "    # if only one image, put it in train\n",
    "    if len(inds) < 2:\n",
    "        train_idx += inds\n",
    "        continue\n",
    "\n",
    "    # 75% train, 25% to split val/test\n",
    "    tr, rest = train_test_split(inds, train_size=0.75, random_state=42)\n",
    "    # split remaining 50/50 for val/test\n",
    "    if len(rest) == 1:\n",
    "        val, test = rest, []\n",
    "    else:\n",
    "        val, test = train_test_split(rest, train_size=0.5, random_state=42)\n",
    "\n",
    "    train_idx += tr\n",
    "    val_idx   += val\n",
    "    test_idx  += test\n",
    "\n",
    "print(f\"→ {len(train_idx)} train, {len(val_idx)} val, {len(test_idx)} test indices\")\n",
    "\n",
    "def annotate_and_save_multi(idx, root_img=IMG_DIR, root_mask=MASK_DIR):\n",
    "    pil_img = ds[idx][\"image\"]\n",
    "    img     = np.array(pil_img)\n",
    "    H, W    = img.shape[:2]\n",
    "    bboxes  = []\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Image #{idx}: draw boxes, ENTER when done\")\n",
    "\n",
    "    def onselect(e0, e1):\n",
    "        x1,y1 = int(e0.xdata), int(e0.ydata)\n",
    "        x2,y2 = int(e1.xdata), int(e1.ydata)\n",
    "        bboxes.append([x1,y1,x2,y2])\n",
    "        rect = plt.Rectangle((x1,y1), x2-x1, y2-y1,\n",
    "                             edgecolor=\"yellow\", fill=False, lw=2)\n",
    "        ax.add_patch(rect)\n",
    "        fig.canvas.draw()\n",
    "\n",
    "    selector = RectangleSelector(ax, onselect, drawtype=\"box\",\n",
    "                                 useblit=True, button=[1],\n",
    "                                 minspanx=5, minspany=5)\n",
    "    # close on ENTER\n",
    "    fig.canvas.mpl_connect(\n",
    "        \"key_press_event\",\n",
    "        lambda ev: plt.close(fig) if ev.key == \"enter\" else None\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    if not bboxes:\n",
    "        print(\"no boxes drawn → skipping\")\n",
    "        return\n",
    "\n",
    "    # preprocess & encode once\n",
    "    img1024 = transform.resize(\n",
    "        img, (1024,1024),\n",
    "        order=3, preserve_range=True, anti_aliasing=True\n",
    "    ).astype(np.uint8)\n",
    "    norm = (img1024 - img1024.min()) / np.clip(\n",
    "        img1024.max() - img1024.min(), 1e-8, None\n",
    "    )\n",
    "    tensor = (\n",
    "        torch.tensor(norm)\n",
    "             .float()\n",
    "             .permute(2,0,1)\n",
    "             .unsqueeze(0)\n",
    "             .to(device)\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        embedding = medsam_model.image_encoder(tensor)\n",
    "\n",
    "    # run & save one mask per box\n",
    "    for i, box in enumerate(bboxes):\n",
    "        box1024 = np.array([box]) / np.array([W, H, W, H]) * 1024\n",
    "        mask = medsam_inference(medsam_model, embedding, box1024, H, W)\n",
    "        base = f\"{idx:04d}_{i:02d}.png\"\n",
    "        pil_img.save(os.path.join(root_img,  base))\n",
    "        Image.fromarray((mask*255).astype(\"uint8\")) \\\n",
    "             .save(os.path.join(root_mask, base))\n",
    "        print(\"✓ saved\", base)\n",
    "\n",
    "# ─── USAGE ────────────────────────────────────────────────────────────────────\n",
    "# annotate your train set:\n",
    "for idx in train_idx:\n",
    "    annotate_and_save_multi(idx)\n",
    "\n",
    "# later you can repeat for val/test by swapping ROOT, IMG_DIR, MASK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the next cell, annotate multiple objects on image 0\n",
    "annotate_and_save_multi(0)# draw multiple boxes, ENTER to finish\n",
    "\n",
    "# then when that finishes, call for image 1, 2, …\n",
    "#annotate_and_save_multi(1)# then move on to image #1, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
