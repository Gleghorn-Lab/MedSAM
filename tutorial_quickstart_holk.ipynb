{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment Anything in Medical Images ([colab](https://colab.research.google.com/drive/1N4wv9jljtEZ_w-f92iOLXCdkD-KJlsJH?usp=sharing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -U\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the local device:\n",
    "- Create a fresh environment `conda create -n medsam python=3.10 -y` and activate it `conda activate medsam`\n",
    "- Install \n",
    "pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126\n",
    "- Continue to next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-trained model\n",
    "\n",
    "Please download the checkpoint [here](https://drive.google.com/drive/folders/1ETWmi4AiniJeWOt6HAsYgTjYv_fkgzoN?usp=drive_link). This pre-trained model can be directed loaded with SAM's checkpoint loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "from segment_anything import sam_model_registry\n",
    "from utils.demo import BboxPromptDemo\n",
    "MedSAM_CKPT_PATH = \"/home/medsam-vit-b/medsam_vit_b.pth\"\n",
    "device = \"cuda:0\"\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "medsam_model = medsam_model.to(device)\n",
    "medsam_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import RectangleSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from medsam_inference import medsam_inference\n",
    "\n",
    "\n",
    "# ─── LOAD & CONCAT ALL HF SPLITS ─────────────────────────────────────────────\n",
    "DATASET_NAME = \"GleghornLab/full_LN_6-1\"\n",
    "raw = load_dataset(DATASET_NAME, token=True)\n",
    "if isinstance(raw, DatasetDict):\n",
    "    parts = []\n",
    "    for split_name, ds_split in raw.items():\n",
    "        ds_split = ds_split.add_column(\"section\", [split_name]*len(ds_split))\n",
    "        parts.append(ds_split)\n",
    "    ds = concatenate_datasets(parts)\n",
    "else:\n",
    "    ds = raw\n",
    "print(\"Total examples:\", len(ds))\n",
    "\n",
    "# ─── GLOBAL RANDOM SPLIT 75/12.5/12.5% ────────────────────────────────────\n",
    "all_idx = list(range(len(ds)))\n",
    "train_idx, rest   = train_test_split(all_idx, train_size=0.75, random_state=42)\n",
    "val_idx, test_idx = train_test_split(rest,   train_size=0.5,  random_state=42)\n",
    "print(f\"→ {len(train_idx)} train, {len(val_idx)} val, {len(test_idx)} test examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.demo import BboxPromptDemo\n",
    "import os\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "\n",
    "\n",
    "# Make output folders if not already present\n",
    "BASE = \"/home/MedSAM/data/follicle\"\n",
    "ROOTS = {\n",
    "    \"train\": BASE + \"_train\",\n",
    "    \"val\":   BASE + \"_val\",\n",
    "    \"test\":  BASE + \"_test\",\n",
    "}\n",
    "for split, root in ROOTS.items():\n",
    "    for sub in (\"images\", \"masks\"):\n",
    "        os.makedirs(os.path.join(root, sub), exist_ok=True)\n",
    "\n",
    "# Create the demo object\n",
    "bbox_prompt_demo = BboxPromptDemo(medsam_model)\n",
    "\n",
    "# Loop through your splits and annotate\n",
    "\n",
    "for split, idxs in [(\"train\", train_idx), (\"val\", val_idx), (\"test\", test_idx)]:\n",
    "    print(f\"\\n=== {split.upper()} ({len(idxs)} images) ===\")\n",
    "    for idx in idxs:\n",
    "        img = ds[idx][\"image\"]  # PIL image\n",
    "        print(f\"Annotating {split} image #{idx}\")\n",
    "        # Save to a temporary file\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp:\n",
    "            img.save(tmp.name)\n",
    "            mask = bbox_prompt_demo.show(tmp.name)  # Pass file path\n",
    "\n",
    "        # Save the image and mask if a mask was returned\n",
    "        if mask is not None:\n",
    "            name = f\"{idx:04d}.png\"\n",
    "            img.save(os.path.join(ROOTS[split], \"images\", name))\n",
    "            Image.fromarray((mask * 255).astype(\"uint8\")).save(os.path.join(ROOTS[split], \"masks\", name))\n",
    "            print(f\"✓ saved {name} (image & mask) to {ROOTS[split]}\")\n",
    "        else:\n",
    "            print(\"⚠ No mask returned, skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "# from skimage import transform\n",
    "# from matplotlib.widgets import RectangleSelector\n",
    "\n",
    "\n",
    "# # ─── MAKE OUTPUT FOLDERS ─────────────────────────────────────────────────────\n",
    "# BASE = \"/home/MedSAM/data/follicle\"\n",
    "# ROOTS = {\n",
    "#     \"train\": BASE + \"_train\",\n",
    "#     \"val\":   BASE + \"_val\",\n",
    "#     \"test\":  BASE + \"_test\",\n",
    "# }\n",
    "# for split, root in ROOTS.items():\n",
    "#     for sub in (\"images\", \"masks\"):\n",
    "#         os.makedirs(os.path.join(root, sub), exist_ok=True)\n",
    "\n",
    "# # ─── ANNOTATION FUNCTION ────────────────────────────────────────────────────\n",
    "# def annotate_and_save_multi(idx, root_img, root_mask):\n",
    "#     # close any leftover figures\n",
    "#     plt.close(\"all\")\n",
    "\n",
    "#     pil_img = ds[idx][\"image\"]\n",
    "#     img = np.array(pil_img)\n",
    "#     H, W = img.shape[:2]\n",
    "#     bboxes = []\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(6,6))\n",
    "#     ax.imshow(img)\n",
    "#     ax.set_title(f\"Image #{idx}: draw boxes, ENTER when done\")\n",
    "\n",
    "#     def onselect(e0, e1):\n",
    "#         x1,y1 = int(e0.xdata), int(e0.ydata)\n",
    "#         x2,y2 = int(e1.xdata), int(e1.ydata)\n",
    "#         bboxes.append([x1,y1,x2,y2])\n",
    "#         ax.add_patch(plt.Rectangle((x1,y1), x2-x1, y2-y1,\n",
    "#                                    edgecolor=\"yellow\", fill=False, lw=2))\n",
    "#         fig.canvas.draw()\n",
    "\n",
    "#     selector = RectangleSelector(\n",
    "#         ax, onselect,\n",
    "#         interactive=True,\n",
    "#         useblit=True,\n",
    "#         button=[1],\n",
    "#         minspanx=5, minspany=5\n",
    "#     )\n",
    "#     fig.canvas.mpl_connect(\n",
    "#         \"key_press_event\",\n",
    "#         lambda ev: plt.close(fig) if ev.key==\"enter\" else None\n",
    "#     )\n",
    "#     plt.show()\n",
    "\n",
    "#     if not bboxes:\n",
    "#         print(\"⚠ no boxes → skipped\")\n",
    "#         return\n",
    "\n",
    "#     # preprocess & encode once\n",
    "#     img1024 = transform.resize(\n",
    "#         img, (1024,1024),\n",
    "#         order=3, preserve_range=True, anti_aliasing=True\n",
    "#     ).astype(np.uint8)\n",
    "#     norm = (img1024 - img1024.min()) / np.clip(\n",
    "#         img1024.max() - img1024.min(), 1e-8, None\n",
    "#     )\n",
    "#     tensor = (\n",
    "#         torch.tensor(norm)\n",
    "#              .float()\n",
    "#              .permute(2,0,1)\n",
    "#              .unsqueeze(0)\n",
    "#              .to(device)\n",
    "#     )\n",
    "#     with torch.no_grad():\n",
    "#         embedding = medsam_model.image_encoder(tensor)\n",
    "\n",
    "#     # run & save one mask per box\n",
    "#     for i, box in enumerate(bboxes):\n",
    "#         box1024 = np.array([box]) / np.array([W, H, W, H]) * 1024\n",
    "#         mask = medsam_inference(medsam_model, embedding, box1024, H, W)\n",
    "\n",
    "#         name = f\"{idx:04d}_{i:02d}.png\"\n",
    "#         pil_img.save(os.path.join(root_img, name))\n",
    "#         Image.fromarray((mask * 255).astype(\"uint8\")) \\\n",
    "#              .save(os.path.join(root_mask, name))\n",
    "#         print(f\"✓ saved {name} → {root_mask}\")\n",
    "\n",
    "# # ─── PROMPT & ANNOTATE SPLITS ────────────────────────────────────────────────\n",
    "# bbox_prompt_demo = BboxPromptDemo(medsam_model)\n",
    "\n",
    "# for split, idxs in [(\"train\", train_idx), (\"val\", val_idx), (\"test\", test_idx)]:\n",
    "#     print(f\"\\n=== {split.upper()} ({len(idxs)} images) ===\")\n",
    "#     for idx in idxs:\n",
    "#         img = ds[idx][\"image\"]\n",
    "#         print(f\"Annotating {split} image #{idx}\")\n",
    "#         bbox_prompt_demo.show(img)\n",
    "# # \n",
    "# #  for split, idxs in [(\"train\", train_idx),\n",
    "# #                     (\"val\",   val_idx),\n",
    "# #                     (\"test\",  test_idx)]:\n",
    "# #     print(f\"\\n=== {split.upper()} ({len(idxs)} images) ===\")\n",
    "# #     for idx in idxs:\n",
    "# #         input(f\"\\nPress ENTER to annotate {split} image #{idx} \")\n",
    "# #         annotate_and_save_multi(\n",
    "# #             idx,\n",
    "# #             os.path.join(ROOTS[split], \"images\"),\n",
    "# #             os.path.join(ROOTS[split], \"masks\")\n",
    "# #         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
